>*Any sufficiently advanced technology is indistinguishable from magic.*
>--Arthur C. Clarke[^1]


Powerful undead wizards within the lore of Dungeons and Dragons are known as a lich. They are creatures who have overcome the limits of their own mortality through magical and technological means. The process of turning oneself into a lich requires the creation of a powerful artifact known as a phylactery[^2], in which the soon to be Lich stores their distilled essence. This process often takes the majority of their natural life to achieve, as both the knowledge and financial cost to construct such a device is extensive[^3]. Upon the creation of this vessel, the lich enters a state of immortal undeath - free to pursue their research and inevitably lose whatever humanity they once had. 

What the lore here is describing is in a sense a form of ultimate archive, even a form of digitization. Through sets of coded instructions, would it be possible to transcend into a form of digital undeath? The source code of any algorithm could be considered at its core a form of magic, a set of instructions for digital incantation. The very nature of coding languages include hints to this magic, from hexspeak to “heavy wizardry”[^4]. I would argue that an artist’s practice is not so dissimilar from a complex algorithm. I offer this text as a primer for my practice. As a means of creating my own algorithm or phylactery, with the hopes that others might follow. How this might be accomplished is subjective to each individual, as are the reasons for doing so. However as any true wizard or research creator might attest, time is fleeting, and data is precarious. Only by archiving, summarizing and sharing might the research continue. 

---
**ARTIFICIAL AESTHETICS**

Let us first consider aesthetics. Computers are simple processing machines, they offer minimal creativity on their own. Even the most up to date stable diffusion AI must be trained[^5]. While at first they feel creative, the images they create are predictable. We depend on predictable results for automated outputs. In 1957, Russell Kirsch developed a method of digitizing an image, first by digitizing a photograph of his infant son Walden. The result, in contrast to the source image, is mired with pixel artifacting and noise associated with the mechanical process. Transferring the analog world to the strict rule set of a binary one results in an artificial aesthetic. When digitizing images we are limited by the fidelity of technology at the time. Compression and translation are constantly improving, however they still are subject to 0’s and 1’s. Aesthetic choices of the digital world are not voluntary. Pixels are static hard edge blocks which only at a greater distance appear smooth. These are often the first things we notice about a “poor image”[^6], either due to generation loss[^7] or image compression. There are some ways around this, or even ways to incorporate human creativity into this process. In 1965, a camera system designed by NASA/JPL, thanks in part to Kirsch’s early experiments, flew to Mars to capture the first ever digital image of another planet. After receiving the first snippets of data, the team at JPL used a pastel set from a nearby art supply store to hand-color a numerical printout of the raw pixel values.

<p align="center"><img width="600" height="459" src="images/First_Image_Coloring.jpg"></p>
><em>A member of the JPL team hand-rendering the first image of Mars with a pastel crayon.</em>

At the time this was much faster than waiting for the image data to be computer processed. The results were a melding of human and machine, with little relation to its photographic counterpart. In Kirsch’s digitized image, Artificial Aesthetics are far more recognizable. Data itself is the purest form of this aesthetic. We are offered a window into the inner workings of a digital image by visualizing this data. By intervening in the process of these images, such as the Mars image, we can creatively shape their predictable outputs. We’re able to control the intent of the image, deciding how artificial it can be. Datamoshing for example, is a type of intentional manipulation of digital video. More associated with a Glitch Art aesthetic, it takes advantage of image compression artifacting to create its visual style. 

![https://vimeo.com/201506064](images/i-frame-finland2016.JPG)
>*Signal Loss [forest], 2017 - Shot during my time at Arteles in 2016, simulating memory degradation or signal loss using i-frame manipulation. https://vimeo.com/201506064

By mixing two or more video sources and by manipulating their i-frames[^8], a single video can appear to meld through a form of digital snow[^9] to reveal the other. Glitch aesthetic is based on nothing more than a surprising outcome from a reliable process. In this case datamoshing is attempting to simulate an unintended effect of digital video. Making these aesthetics intentionally more apparent.

Contemporary painting has always seemed to focus on accuracy over intent[^10]. However the human eye and reality itself is subjective. Historically, painting has offered a unique means of representing reality[^11]. With intent not always superseding accuracy. The Hockney-Falco thesis argues Renaissance paintings from the 1500’s onwards were made with the extensive use of lenses as a way of achieving the most accurate images possible. Mostly these are perfect replications of reality[^11]. So perfect in fact that in a few examples clear evidence of photographic imperfections are present. Most notably in the 1523 Lorenzo Lotto painting, “Family Portrait''. We see part of a tablecloth dip in and out of focus. Something the human eye simply cannot do as it focuses on a subject.

![Family Portrait [detail], Lorenzo Lotto, 1523](images/Lorenzo Lotto - Family Portrait [detail].JPG)
>*Family Portrait [detail], Lorenzo Lotto, 1523

Living in 1523, with the photograph still 300 years away I wonder if I would have the same reaction to this painting that I have towards computer generated imagery? Similarly, the painting “Old Woman'' by Georges de la Tour in 1619 seems skewed in some way. As if a projection the artist was possibly working from was hitting the surface of the canvas at a slight angle. The woman's legs in the painting seem stretched, as if their legs were disproportionally longer than their body. This type of optical distortion is so minor that a modern day viewer[^12] might not pay it any mind, yet something about these images appears unsettling. What effect does artificial aesthetics have on the viewer? Perhaps the greatest example is “the uncanny valley”. The aesthetic concept that relates to the emotional reaction or even revulsion against a human observer’s affinity for a replica. Specifically, the more such a replica begins to approach “real”, almost right up until it might become “correct”. Good design attempts to mitigate this effect with the use of skeuomorphs[^13], often used to make something new feel familiar. Usually though, we find these attempts at reality disturbing. The quest for perfected simulated reality continues. 

In 2015, the first entries into machine learning began making startling progress. The actual training of a program to direct and manipulate certain datasets. Able to entirely generate new and accurate images. Recently stable diffusion, generative adversarial networks (GAN), and source code offered by OpenAI has made it incredibly easy for the average user to train and generate their own images. For instance, The Image Generator[^14] project by Greg Surma attempts to generate new Simpsons characters by crawling through every frame of the show[^15] and outputs its interpreted results. Casey Reas[^16], uses a similar concept in his recent work. Using every frame of a film, Reas is able to generate additional (albeit horrifying) frames of these films. Also, with the growing availability of “DeepFake'' technology, artists are able to take

this process and apply it to the reskinning of human subjects. Thanks in part to older films or the wealth of images available on the internet, frames of an actor's face or well documented historical figure are compiled into a slowly rotating scan of their features. Autoencoders or GANs are then able to match these frames to a structure - an original video file. The result is a video of an original subject replaced with someone else’s likeness. These are early days for this type of technology, yet the concepts are there. Due to the limitations of processing power, the results are often easy to spot. Plunging face-first into the uncanny valley. However as graphics cards and rendering nodes are becoming cheaper and processing power itself is becoming more accessible, this technology is becoming far harder to detect. Almost to the point of invisibility in some cases. This could lead to a very confusing foreseeable future without proper media awareness. 

Stable diffusion models have produced a wave of AI generated art. Interestingly in most cases these have been centered on “creativity” rather than reality or accuracy. As a means to quickly populate these models with content to train from, online art communities such as DeviantArt and Behance were crawled and uploaded. The results of these AI generators are still predictable. Images that look and feel creative, but are no more than plagiarized illustrations of others' artists work. Perhaps as a reaction towards recent backlash from artists and the illustration community, newer stable diffusion models have begun to pop up. These are trained using stock photos, historical photographs, or even some capable of using a user submitted image as a starting reference.

![Fake misplaced test footage of the 80s “MATRIX“ Version by Alejandro Jodorowsky. Tommy Lee Jones as Agent Smith. Made by Infinite Odyssey Magazine using stable diffusion [Midjourney].](images/infiniteodyssey.mag - matrix AI.JPG)

[^1]: Please use these footnotes as a bit of a recurring chat window. Or a resource for terms to search for. 
[^2]: Urns, rings, wands, dolls, any object or even a family line could be used as a phylactery. 
[^3]: Wizards of the Coast, Inc, editor. Monster Manual. 5th edition, Wizards Of The Coast, 2014. -- states this ritual has a material cost of 120,000 gold pieces. Worth approximately $37,320,000 today. 
[^4]: Referring to code based on obscure or undocumented intricacies of a particular hardware or software.
[^5]: Image based AI models are fed hundreds or thousands of pre-existing images in order to generate an output.
[^6]; https://www.e-flux.com/journal/10/61362/in-defense-of-the-poor-image/
[^7]: Copying of a copy over hundreds of generations
[^8]: Intermediate Frames
[^9]: TV snow/noise indicating no signal
[^10]: Historically anyway.
[^11]: Van Eyck, Jan. The Arnolfini Wedding. 1434.
[^12]: Familiar with photography and its properties.
[^13]: Derivative objects that retain ornamental design from structures that were necessary in the original.
[^14]: https://towardsdatascience.com/image-generator-drawing-cartoons-with-generative-adversarial-networks-45e814ca9b6b
[^15]: https://frinkiac.com/
[^16]: One of the creators of Processing - a visual coding language directed towards designers and artists.
